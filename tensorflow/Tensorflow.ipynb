{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tensroflow logo](Images\\TensorFlowLogo.png)\n",
    "**TensorFlow** is an open source software library for machine learning across a range of tasks, and developed by Google to meet their needs for systems capable of building and training neural networks to detect and decipher patterns and correlations, analogous to the learning and reasoning which humans use. It is currently used for both research and production at Google products.\n",
    "\n",
    "TensorFlow provides **multiple APIs**. The lowest level API **TensorFlow Core** provides you with complete programming control. We recommend TensorFlow Core for machine learning researchers and others who require fine levels of control over their models. The higher level APIs are built on top of TensorFlow Core. These higher level APIs are typically easier to learn and use than TensorFlow Core. In addition, the higher level APIs make repetitive tasks easier and more consistent between different users. A high-level API like tf.contrib.learn helps you manage data sets, estimators, training and inference. Let's just import all the necessary stuff required for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3                                       # a rank 0 tensor; this is a scalar with shape []\n",
    "[1. ,2., 3.]                            # a rank 1 tensor; this is a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]]            # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]]        # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Computational Graph\n",
    "A computational graph is a series of TensorFlow operations arranged into a graph of nodes. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constants, it takes no inputs, and it outputs a value it stores internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node1: Tensor(\"Const_4:0\", shape=(), dtype=float32) \n",
      "Node2: Tensor(\"Const_5:0\", shape=(), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0,tf.float32)                  # A floating type constant\n",
    "node2 = tf.constant(4, tf.uint8)                     # An integer type constant\n",
    "print('Node1: {} \\nNode2: {}'.format(node1,node2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that printing the nodes does not output the values 3.0 and 4.0 as you might expect. Instead, they are nodes that, when evaluated, would produce 3.0 and 4.0, respectively. **To actually evaluate the nodes, we must run the computational graph within a session. A session encapsulates the control and state of the TensorFlow runtime.** The following code creates a Session object and then invokes its run method to run enough of the computational graph to evaluate node1 and node2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()                                  # Creating Session object\n",
    "print(sess.run([node1,node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build more complicated computations by combining Tensor nodes with operations. For example-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: 7.0\n"
     ]
    }
   ],
   "source": [
    "Adder = tf.add(node1,tf.cast(node2, tf.float32))     # Notice the casting operation being performed here\n",
    "print('Sum: {}'.format(sess.run(Adder)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as **placeholders**. A placeholder is a promise to provide a value later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.724105  , -0.65286815],\n",
       "       [-0.1504311 ,  0.19988585]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32)                      # A placeholder does not accept a tensor as a input\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "X_train = np.random.randn(1,1).astype(np.float32)    \n",
    "y_train = np.random.randn(2,2).astype(np.float32)\n",
    "\n",
    "reduce = X + y\n",
    "sess.run(reduce,{X:X_train, y:y_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, we create models and then we evaluate them to test how they peform on training and testing data. Loss functions are the measures, to observe how our model is learning over period of time. To understand this concept, let us first create a model and then perform evaluation on it. For this tutorial lets create a not gate using tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.random.randint(0,2,(5,1)).astype(np.float32)    # Training data\n",
    "y_train = np.logical_not(X_train).astype(np.float32)         # Training labels\n",
    "\n",
    "# Model parameter\n",
    "X = tf.placeholder(tf.float32)                                \n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1,1]), tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1,1]), tf.float32)\n",
    "\n",
    "model = W * X + b\n",
    "\n",
    "# mean square error loss\n",
    "loss = tf.reduce_sum(tf.square(model - y))                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined a loss function, let's use one of the many in-build optimizers for reducing loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual output: \n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]] \n",
      "Predicted output: \n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)      # Gradient Decent optimizer\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()                 # initializing global variables\n",
    "sess.run(init)                                           # Note: this is very important step \n",
    "\n",
    "# Training loop\n",
    "for i in range(100):\n",
    "    sess.run(train,{X:X_train, y:y_train})\n",
    "\n",
    "print('actual output: \\n{} \\nPredicted output: \\n{}'.format(y_train, np.round(sess.run(model,{X:X_train, y:y_train}))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly observe that, when rounded our model predicted all the values correctly. So, we have learnt the correct hyperparameters for this particular model. Let's look at the value-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix: [[-0.43818688]] \n",
      "Bais: [[ 0.59506792]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weight Matrix: {} \\nBais: {}\".format(sess.run(W), sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move on the \"hello world\" program in machine learning, the **MNIST** data set. It is kind of **benchmark** for evaluation of any machine learning model. MNIST is a simple computer vision dataset. It consists of images of handwritten digits like these:\n",
    "\n",
    "![MNIST data](Images/MNIST_example.png)\n",
    "\n",
    "It also includes labels for each image, telling us which digit it is.For example, the labels for the above images are 5, 0, 4, and 1.\n",
    "\n",
    "The MNIST data is split into three parts: 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation). This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(label, no_class):\n",
    "    one_hot_labels = np.zeros((label.shape[0],no_class))\n",
    "    one_hot_labels[np.arange(label.shape[0]),label] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMnist(path):\n",
    "    fp = open(path,'rb')\n",
    "    fileData = pickle.load(fp, encoding='latin1')\n",
    "    feature_vector = fileData[0][0]\n",
    "    label = oneHot(fileData[0][1],10)\n",
    "    return(feature_vector, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nX = tf.placeholder(tf.float32)\\ny = tf.placeholder(tf.float32)\\n\\nW = tf.Variable(tf.random_normal([784,10]),tf.float32)\\nb = tf.Variable(tf.random_normal([1,10]),tf.float32)\\n\\nmodel = tf.matmul(X,W) + b\\n\\nsoftmax = tf.nn.softmax(model)\\n\\nloss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(softmax),axis=1))\\n\\noptimizer = tf.train.GradientDescentOptimizer(0.5)\\ntrain = optimizer.minimize(loss)\\n\\nsess = tf.Session()\\nsess.run(tf.global_variables_initializer())\\n\\nfor _ in range(50000):\\n    random_sequence = np.random.randint(0,50000,(100))\\n    feature = feature_vector[random_sequence,:]\\n    labell = labels[random_sequence,:]\\n    sess.run(train,{X:feature, y:labell})\\n    \\n#sess.run(softmax,{X:feature_vector,y:labels})\\npred = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(y,1),tf.argmax(softmax,1)),tf.float32))\\n\\nprint('Accuracy achieved: {}% '.format(np.round((sess.run(pred,{X:feature_vector,y:labels})*100)),2))\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_class = 10\n",
    "#fp = open('C:\\\\Users\\\\pradeepy\\\\Anconda Notebook\\\\Images\\\\mnist.pkl','rb')\n",
    "#filedata = pickle.load(fp, encoding='latin1')\n",
    "\n",
    "#feature_vector = filedata[0][0]\n",
    "#label = filedata[0][1]\n",
    "\n",
    "#labels = oneHot(label, no_class)\n",
    "path = 'C:\\\\Users\\\\pradeepy\\\\Anconda Notebook\\\\Images\\\\mnist.pkl'\n",
    "feature_vector, labels = loadMnist(path)\n",
    "\n",
    "print(feature_vector.shape)\n",
    "'''\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784,10]),tf.float32)\n",
    "b = tf.Variable(tf.random_normal([1,10]),tf.float32)\n",
    "\n",
    "model = tf.matmul(X,W) + b\n",
    "\n",
    "softmax = tf.nn.softmax(model)\n",
    "\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(softmax),axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for _ in range(50000):\n",
    "    random_sequence = np.random.randint(0,50000,(100))\n",
    "    feature = feature_vector[random_sequence,:]\n",
    "    labell = labels[random_sequence,:]\n",
    "    sess.run(train,{X:feature, y:labell})\n",
    "    \n",
    "#sess.run(softmax,{X:feature_vector,y:labels})\n",
    "pred = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(y,1),tf.argmax(softmax,1)),tf.float32))\n",
    "\n",
    "print('Accuracy achieved: {}% '.format(np.round((sess.run(pred,{X:feature_vector,y:labels})*100)),2))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting 93% accuracy on MNIST is bad. It's almost embarrassingly bad. In this section, we'll fix that, jumping from a very simple model to something moderately sophisticated: a small convolutional neural network. This will get us to around 99.2% accuracy which is not state of the art, but respectable. Lets directly jump to the code and see how different part of the code works.\n",
    "\n",
    "**Note-**The convolution operation takes up a lot of memory, so this code might take up to 15 min before giving out the achieved accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# Let us start by loading the data into memory from a pickled file\n",
    "###################################################################\n",
    "fp = open('C:\\\\Users\\\\pradeepy\\\\Anconda Notebook\\\\Images\\\\mnist.pkl','rb')\n",
    "filedata = pickle.load(fp, encoding='latin1')\n",
    "feature_vector = filedata[0][0]\n",
    "label = filedata[0][1]\n",
    "one_hot_label = np.zeros((label.shape[0],10))                    # Creating one-hot vector for the labels\n",
    "one_hot_label[np.arange(label.shape[0]),label] = 1\n",
    "\n",
    "\n",
    "############################################################################\n",
    "# lets move on and build our very first convolutional neural network model\n",
    "############################################################################\n",
    "\n",
    "# Here is our placeholders for feature vector and labels required for learning\n",
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# first we need to reshape our original image to a rank-4 tensor for performing convolution operation\n",
    "X_image = tf.reshape(X, [-1,28,28,1])\n",
    "\n",
    "#########################\n",
    "# First Convolution layer\n",
    "#########################\n",
    "\n",
    "# Weight and Bais initialization\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32],stddev=0.1)) # truncated_normal discards any value outside greater or less \n",
    "b_conv1 = tf.Variable(tf.constant(0.1,shape = [1,32]))            # than 2*stddev from mean value\n",
    "\n",
    "# performing convolution and maxpool operation one after another\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(X_image, W_conv1, strides=[1,1,1,1], padding='SAME'))   # middle two value discribe stride\n",
    "h_maxpool1 = tf.nn.max_pool(h_conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  # x and y respectively\n",
    "                                                                                          # first value is batch\n",
    "                                                                                          # last is depth of convolution  \n",
    "\n",
    "############################\n",
    "# Second Convolutional Layer\n",
    "############################\n",
    "\n",
    "# Weight and Bias initialization  \n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[1,64]))\n",
    "\n",
    "# performing convolution and maxpool operation one after another\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_maxpool1, W_conv2, strides=[1,1,1,1], padding='SAME'))\n",
    "h_maxpool2 = tf.nn.max_pool(h_conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#############\n",
    "# Dense layer\n",
    "#############\n",
    "\n",
    "# Weight and Bias initialization\n",
    "W_dense = tf.Variable(tf.truncated_normal([7*7*64,1024], stddev=0.1))\n",
    "b_dense = tf.Variable(tf.constant(0.1, shape=[1,1024]))\n",
    "\n",
    "# Introducing a dense layer\n",
    "h_dense = tf.nn.relu(tf.matmul(tf.reshape(h_maxpool2,[-1, 7*7*64]), W_dense) + b_dense)\n",
    "\n",
    "################\n",
    "# Dropout layer\n",
    "################\n",
    "\n",
    "# Creating placeholder for drop-probabilities and performimg dropout operation \n",
    "drop_prob = tf.placeholder(tf.float32)\n",
    "drop_layer = tf.nn.dropout(h_dense, drop_prob)\n",
    "\n",
    "###########################\n",
    "# Creating a softmax layer\n",
    "###########################\n",
    "\n",
    "# Weight and Bias initialization\n",
    "W_softmax = tf.Variable(tf.truncated_normal([1024,10], stddev=0.1))\n",
    "b_softmax = tf.Variable(tf.constant(0.1, shape=[1,10]))\n",
    "\n",
    "# performing matrix multiplication operation and adding bias\n",
    "h_sfmx = tf.matmul(drop_layer, W_softmax) + b_softmax\n",
    "\n",
    "# Defining loss \n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=h_sfmx)  # function automatically calculates softmax and \n",
    "                                                                         # calculates the cross-entropy loss\n",
    "\n",
    "# Note:: We can also use tf.nn.sparse_softmax_cross_entropy_with_logits()  which will automatically create a one-hot encoding\n",
    "#        for your one dimentional label vector\n",
    "    \n",
    "# Defining optimizing function\n",
    "train = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# Node to check-out accuracy of the bulit model\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.arg_max(h_sfmx,1), tf.arg_max(y,1)),tf.float32))\n",
    "\n",
    "# Creating and initilizing session variables\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "# Training the network using batches of 100 images at a time\n",
    "for _ in range(1000):\n",
    "    random_sequence = np.random.randint(0,50000,(100))\n",
    "    feature = feature_vector[random_sequence,:]\n",
    "    labell = one_hot_label[random_sequence,:]\n",
    "    sess.run(train,{X:feature,y:labell,drop_prob:0.5})\n",
    "    \n",
    "# Printing out the accuracy of the built system\n",
    "print('Accuracy: {}'.format(sess.run(accuracy,{X:feature_vector[:5000,:], y:one_hot_label[:5000,:],drop_prob:1.0})))\n",
    "print(sess.run(drop_prob,{drop_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
